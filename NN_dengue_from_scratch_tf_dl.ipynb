{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version 10.0.130\r\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA Version\n",
    "!cat /usr/local/cuda/version.txt;\n",
    "# !nvcc --version\n",
    "# !cat /usr/local/cuda/include/cudnn.h # | grep CUDNN_MAJOR -A 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.4\n",
      "tensorflow==2.0.0\n",
      "tensorflow-datasets==1.2.0\n",
      "tensorflow-estimator==2.0.0\n",
      "tensorflow-gpu==2.0.0\n",
      "tensorflow-hub==0.6.0\n",
      "tensorflow-metadata==0.15.0\n",
      "pandas==0.25.1\n",
      "numpy==1.17.2\n"
     ]
    }
   ],
   "source": [
    "# Check your versions (conda env tf2)\n",
    "!python --version;\n",
    "!pip freeze | grep tensorflow;\n",
    "!pip freeze | grep pandas;\n",
    "!pip freeze | grep numpy;\n",
    "!pip freeze | grep fastai;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.4\n",
      "tensorflow==2.0.0\n",
      "tensorflow-datasets==1.2.0\n",
      "tensorflow-estimator==2.0.0\n",
      "tensorflow-gpu==2.0.0\n",
      "tensorflow-hub==0.6.0\n",
      "tensorflow-metadata==0.15.0\n",
      "pandas==0.25.1\n",
      "numpy==1.17.2\n"
     ]
    }
   ],
   "source": [
    "# Check your versions (conda env tf2)\n",
    "!python --version;\n",
    "!pip freeze | grep tensorflow;\n",
    "!pip freeze | grep pandas;\n",
    "!pip freeze | grep numpy;\n",
    "!pip freeze | grep fastai;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# from fastai.tabular import *\n",
    "# from fastai.collab import *\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import datetime\n",
    "import typing\n",
    "import numbers\n",
    "import os\n",
    "import unittest\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Ensure training on one GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../mydata')\n",
    "filename='patients_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "splits = 5 # number of folds for cross validation\n",
    "recalls = [0.85,0.90,0.95] # sensitivities used for calculating results\n",
    "\n",
    "\n",
    "# Imported columns from CSV\n",
    "desired_cols = ['age','sex','Temp','exam_WBC','exam_Plt', 'Opd_Visit_Date',\n",
    "                'ER', 'Heart Disease', 'CVA', 'CKD', 'Severe Liver Disease', \n",
    "                'DM', 'Hypertension', 'Cancer without Metastasis', 'Cancer with Metastasis',\n",
    "                'lab_result']\n",
    "\n",
    "# Features used for training + dependent variable\n",
    "train_cols = ['age','Temp','exam_WBC','exam_Plt','lab_result']\n",
    "\n",
    "\n",
    "# Features used for creating validation subgroups (includes features from train_cols)\n",
    "subgroup_cols = ['age','sex','Temp','exam_WBC','exam_Plt', 'week',\n",
    "                'ER', 'Heart Disease', 'CVA', 'CKD', 'Severe Liver Disease', \n",
    "                'DM', 'Hypertension', 'Cancer without Metastasis', 'Cancer with Metastasis',\n",
    "                'lab_result']\n",
    "\n",
    "cont_cols = ['age','Temp','exam_WBC','exam_Plt']\n",
    "cat_cols = []\n",
    "\n",
    "# Columns to be dropped after creating validation subgroups\n",
    "drop_cols = list(set(subgroup_cols) - set(train_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DF\n",
    "\n",
    "df = pd.read_csv(path/filename, usecols=desired_cols)\n",
    "df = df.iloc[np.random.permutation(len(df))] # randomize data\n",
    "\n",
    "# Convert Opd_Visit_Date to week of year format\n",
    "if 'week' not in df.columns and 'Opd_Visit_Date' in df.columns:\n",
    "    week_numbers = [int(datetime.datetime.strptime(d, \"%Y/%m/%d\").strftime(\"%U\"))+1 for d in df['Opd_Visit_Date']]\n",
    "    df.insert(0, 'week', week_numbers)\n",
    "    df.drop(columns=['Opd_Visit_Date'],inplace=True)\n",
    "    \n",
    "# Convert 男 and 女 to 0 and 1 in column 'sex'\n",
    "df_male_indx = df[df['sex']=='男']\n",
    "df_female_indx = df[df['sex']=='女']\n",
    "\n",
    "for i in df_male_indx.index.tolist():\n",
    "    df.at[int(i),'sex'] = 0\n",
    "for i in df_female_indx.index.tolist():\n",
    "    df.at[int(i),'sex'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "\n",
    "# Equal length subsets of original dataframe\n",
    "len_df = len(dataframe)\n",
    "cut_indices = [int(i*(1/splits)*len_df) for i in range(0,splits+1)]\n",
    "cut_indices = zip(cut_indices[:-1], cut_indices[1:])\n",
    "subsets = [dataframe[i:j] for i,j in cut_indices]\n",
    "\n",
    "valids = subsets\n",
    "trains = [pd.concat(subsets[1:], axis=0)]\n",
    "for n in range(1,splits):\n",
    "    trains += [pd.concat(subsets[:n]+subsets[n+1:], axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tests\n",
    "assert len(valids[0])+len(trains[0])==len(df)\n",
    "assert list(valids[0].index) == list(valids[0].index)\n",
    "assert list(valids[1].index) != list(valids[0].index)\n",
    "assert list(trains[1].index) != list(trains[0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Dataframe into Subgroups\n",
    "\n",
    "frames = [] # temporarily stores dataframes\n",
    "subgroup_dicts = []\n",
    "\n",
    "for modelnum in range(1,splits+1):\n",
    "\n",
    "    dataframe = subsets[modelnum-1] # validation set\n",
    "\n",
    "    # age\n",
    "    df_age_under_18 = dataframe[dataframe['age']<18]\n",
    "    df_age_18_to_65 = dataframe[(dataframe['age']>=18) & (dataframe['age']<65)]\n",
    "    df_age_over_eq_65 = dataframe[dataframe['age']>=65]\n",
    "\n",
    "    # sex\n",
    "    df_female = dataframe[dataframe['sex']==1]\n",
    "    df_male = dataframe[dataframe['sex']==0]\n",
    "\n",
    "    # week\n",
    "    df_wks_35 = dataframe[dataframe['week']<=35]\n",
    "    df_wks_35_to_40 = dataframe[(dataframe['week']>35) & (dataframe['week']<=40)]\n",
    "    df_wks_over_40 = dataframe[dataframe['week']>40]\n",
    "\n",
    "    # Temp\n",
    "    df_temp_over_eq_38 = dataframe[dataframe['Temp']>=38]\n",
    "    df_temp_under_38 = dataframe[dataframe['Temp']<38]\n",
    "\n",
    "    # exam_WBC\n",
    "    df_wbc_low = dataframe[dataframe['exam_WBC']<=3.2]\n",
    "    df_wbc_normal = dataframe[(dataframe['exam_WBC']>3.2) & (dataframe['exam_WBC']<10)]\n",
    "    df_wbc_high = dataframe[dataframe['exam_WBC']>=10]\n",
    "\n",
    "    # exam_Plt\n",
    "    df_plt_low = dataframe[dataframe['exam_Plt']<100]\n",
    "    df_plt_high = dataframe[dataframe['exam_Plt']>=100]\n",
    "\n",
    "    # Comorbidities\n",
    "    df_heart_disease = dataframe[dataframe['Heart Disease']==True]\n",
    "    df_cva = dataframe[dataframe['CVA']==True]\n",
    "    df_ckd = dataframe[dataframe['CKD']==True]\n",
    "    df_liver = dataframe[dataframe['Severe Liver Disease']==True]\n",
    "    df_dm = dataframe[dataframe['DM']==True]\n",
    "    df_hypertension = dataframe[dataframe['Hypertension']==True]\n",
    "\n",
    "\n",
    "    df_cancer1 = dataframe[(dataframe['Cancer with Metastasis']==True)]\n",
    "    df_cancer2 = dataframe[(dataframe['Cancer without Metastasis']==True)]\n",
    "    df_cancer = pd.concat([df_cancer1, df_cancer2], axis=0)\n",
    "\n",
    "    df_er = dataframe[dataframe['ER']==True]\n",
    "    \n",
    "    overall = dataframe\n",
    "\n",
    "    frame = [df_age_under_18, df_age_18_to_65, df_age_over_eq_65, df_female, df_male, df_wks_35, df_wks_35_to_40, \n",
    "          df_wks_over_40, df_temp_over_eq_38, df_temp_under_38, df_wbc_low, df_wbc_normal, df_wbc_high, \n",
    "          df_plt_low, df_plt_high, df_heart_disease, df_cva, df_ckd, df_liver, df_dm, df_hypertension, \n",
    "          df_cancer, df_er, overall]\n",
    "\n",
    "    dfs_names = ['df_age_under_18', 'df_age_18_to_65', 'df_age_over_eq_65', 'df_female', 'df_male', 'df_wks_35', 'df_wks_35_to_40', \n",
    "          'df_wks_over_40', 'df_temp_over_eq_38', 'df_temp_under_38', 'df_wbc_low', 'df_wbc_normal', 'df_wbc_high', \n",
    "          'df_plt_low', 'df_plt_high', 'df_heart_disease', 'df_cva', 'df_ckd', 'df_liver', 'df_dm', 'df_hypertension', \n",
    "          'df_cancer', 'df_er','overall']\n",
    "\n",
    "    subgroup_dict = {name:frame for (name, frame) in zip(dfs_names, frame)}\n",
    "    \n",
    "#     dfs = frame # because of older code... (technical debt)\n",
    "\n",
    "    # Display Subgroup Sizes\n",
    "\n",
    "#     print(\"Dataframes (Model \"+str(modelnum)+')',\" \"*(20-len('dataframes (Model n)')),\"| Length\", ' ',\"| Percent Length\")\n",
    "#     print(\"-\"*50)\n",
    "#     length = 0\n",
    "#     for i in range(len(dfs)):\n",
    "#         print(dfs_names[i],' '*(20-len(dfs_names[i])),'|',len(dfs[i]),' '*(7-len(str(len(dfs[i])))),f'| {100*len(dfs[i])/len(dataframe):0.2f}%')\n",
    "#         length += len(dfs[i])\n",
    "        \n",
    "#     print('-'*42)\n",
    "#     print('valid_df '+str(modelnum),' '*(20-len('valid_df 1')), '|', len(dataframe),' '*2 ,f' | 100%') \n",
    "#     print('\\n'*2)\n",
    "\n",
    "    frames += [frame]\n",
    "    subgroup_dicts += [subgroup_dict]\n",
    "    \n",
    "# RESET DFS\n",
    "\n",
    "dfs = frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilbert/anaconda3/envs/tf2/lib/python3.7/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# drop_cols defined in Parameters section\n",
    "\n",
    "for model_indx in range(len(dfs)):\n",
    "    dataframes = dfs[model_indx]\n",
    "    train_df = trains[model_indx]\n",
    "    valid_df = subsets[model_indx]\n",
    "    \n",
    "    \n",
    "    # Remove columns of unused features in validation subgroups\n",
    "    for i in range(len(dataframes)):\n",
    "        if drop_cols[0] in dataframes[i].columns:\n",
    "            dataframes[i].drop(columns=drop_cols,inplace=True)\n",
    "\n",
    "    # Remove columns of unused features in training dataset\n",
    "    if drop_cols[0] in train_df.columns:\n",
    "        train_df.drop(columns=drop_cols,inplace=True)\n",
    "\n",
    "    # Remove columns of unused features in full validation dataset\n",
    "    if drop_cols[0] in valid_df.columns:\n",
    "        valid_df.drop(columns=drop_cols,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trains[0]\n",
    "valid = valids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32, target='target'):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(target)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train, target='lab_result')\n",
    "valid_ds = df_to_dataset(valid, shuffle=False, target='lab_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for col in cont_cols:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 0.5571 - accuracy: 0.7232 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.7835 - val_loss: 0.6802 - val_accuracy: 0.6421\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4676 - accuracy: 0.7883 - val_loss: 0.5147 - val_accuracy: 0.7597\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4573 - accuracy: 0.7980 - val_loss: 0.4682 - val_accuracy: 0.7986\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.8003 - val_loss: 0.4622 - val_accuracy: 0.7945\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4488 - accuracy: 0.8041 - val_loss: 0.4604 - val_accuracy: 0.7955\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4587 - accuracy: 0.7972 - val_loss: 0.4619 - val_accuracy: 0.8016\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.7965 - val_loss: 0.4595 - val_accuracy: 0.7996\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4441 - accuracy: 0.8100 - val_loss: 0.4577 - val_accuracy: 0.8037\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4493 - accuracy: 0.8018 - val_loss: 0.4569 - val_accuracy: 0.8006\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4509 - accuracy: 0.7998 - val_loss: 0.4534 - val_accuracy: 0.8047\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4463 - accuracy: 0.8085 - val_loss: 0.4537 - val_accuracy: 0.8067\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4504 - accuracy: 0.7985 - val_loss: 0.4533 - val_accuracy: 0.8057\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.7960 - val_loss: 0.4547 - val_accuracy: 0.8037\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.4549 - accuracy: 0.7949 - val_loss: 0.4545 - val_accuracy: 0.8057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe55fe1f510>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = keras.Sequential([\n",
    "    layers.DenseFeatures(feature_columns),\n",
    "    \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(16,activation='relu'),\n",
    "    \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(16,activation='relu'),\n",
    "    \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(16,activation='relu'),\n",
    "    \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', # categorical_crossentropy for multilabel classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=valid_ds,\n",
    "          epochs=15,\n",
    "          callbacks=[tf.keras.callbacks.ReduceLROnPlateau()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features_4 (DenseFeatu multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch multiple                  16        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  80        \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             multiple                  17        \n",
      "=================================================================\n",
      "Total params: 849\n",
      "Trainable params: 745\n",
      "Non-trainable params: 104\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(model, dataframe, target='lab_result'):\n",
    "    # makes a prediction with given model\n",
    "    return model.predict(df_to_dataset(dataframe, shuffle=False, target=target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_targ(dataframe, target='lab_result'):\n",
    "    # returns targets as numpy array for given dataframe\n",
    "    return dataframe[target].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'df_female'\n",
    "preds = make_pred(model,df_female)\n",
    "targets = make_targ(df_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_recall = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = preds\n",
    "fpr, tpr, thresholds = metrics.roc_curve(targets, score, pos_label=1)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(targets, score, pos_label=1)\n",
    "recall = np.asarray(recall)\n",
    "idx = (np.abs(recall - min_recall)).argmin() # Find nearest threshold\n",
    "thresh = thresholds[idx]\n",
    "\n",
    "predict_label = [1 if s >= thresh else 0 for s in range(len(score))]\n",
    "conf_mat = confusion_matrix(targets, predict_label)\n",
    "\n",
    "TN, FP, FN, TP = conf_mat.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 189],\n",
       "       [  0, 295]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Set', 'df_female'),\n",
       " ('sensitivity', 1.0),\n",
       " ('specificity', 0.005263157894736842),\n",
       " ('accuracy', 0.6103092783505155),\n",
       " ('PPV', 0.609504132231405),\n",
       " ('NPV', 1.0),\n",
       " ('F1', 0.7573812580231065),\n",
       " ('odds_ratio', 0),\n",
       " ('TN', 1),\n",
       " ('FP', 189),\n",
       " ('FN', 0),\n",
       " ('TP', 295),\n",
       " ('total', 485)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'df_female'\n",
    "PPV = TP / (TP + FP) if (TP+FP != 0) else 0 # positive predict value\n",
    "NPV = TN / (TN + FN) if (TN+FN != 0) else 0 # negative predict value\n",
    "F1 = 2*TP / (2*TP + FP + FN) #\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "sensitivity = TP /(TP + FN) if (TP+FN != 0) else 0\n",
    "specificity = TN /(TN + FP) if (TN+FP != 0) else 0\n",
    "odds_ratio = (TP * TN) /(FP * FN) if (FP*FN != 0) else 0\n",
    "total = FN+FP+TN+TP\n",
    "\n",
    "\n",
    "result = [name,sensitivity, specificity, accuracy, PPV, NPV, F1, odds_ratio, TN, FP, FN, TP, total]\n",
    "result_titles = ['Set','sensitivity', 'specificity', 'accuracy', 'PPV', 'NPV', 'F1', 'odds_ratio', 'TN', 'FP', 'FN', 'TP', 'total']\n",
    "list(zip(result_titles, result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
